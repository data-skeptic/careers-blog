# Using Machine Learning on Small Datasets

Over the past few decades, the ability to train machine learning models at scale is a promise delivered.  Scaling ML to process millions, billions, or greater number of rows of data is attainable today.  What isn't always attainable is access to millions and billions of rows of data.  Large corporations often have large datasets.  This may not be the case for small and medium sized companies.

What can an organization do when their dataset is only a few dozen rows of data?  Is machine learning still the right tool?

The answer, as with most things, is "it depends".  It's good to be skeptical and start from a position that ML will NOT be helpful with a small dataset.  Remember, ML is not magic.  It is unlikely to make better predictions than a human expert on a small dataset.  Perhaps it would be more fruitful for a human analyst to inspect the data and try to draw conclusions.  ML's value is that a trained model can inference in real time on large volumes of data.  Depending on the question you want to answer with your data, the level of effort might not make ML the right tool.

The fundamental concern when working with small datasets is **overfitting**.  If you have more features (columns) than observations (rows), it is highly likely you will overfit your data.  Such a trained ML model will find a way to "cheat", in which it doesn't learn the underlying distribution of your data, but instead "memorizes" the examples in the training set.  The common retort for this issue is to apply cross validation.

It can be easy to train a model which appears to work well during your training / testing phase, but not generalize well to new data.  [Cross Validation](https://dataskeptic.com/blog/episodes/2014/cross-validation) is an important tool, but when working with a tiny dataset, it is still easy for inexperienced data scientists to overfit, despite a seemingly good cross validation result.  Many problems have a class imbalance.  For example, in fraud detection problems, the vast majority of transactions will not be fraudulent.  A poorly designed model is like a broken clock but worse.  A broken close is right twice per day.  A broken fraud detection model can be right 99% of the time and still be utterly useless.

For important business decisions, or datasets that aren't trivially small, it often is worthwhile the invest R&D time in developing a machine learning model to make some prediction or assessment of your dataset.  For small datasets, you need to be especially careful to avoid torturing your data.  Many data scientists in this situation will encounter pressure from their organization to find some insight in the data, even if there might not in fact be an insight to find.  In these cases, it's important to educate your co-workers about the limits of the numeric techniques available to you.  You may find this more productive if you share exploratory data analysis which expresses the pragmatic findings you can generate.

There's nothing wrong with going on a fishing expedition with your data in search of whatever insights can be found.  But use caution in the interpretation of your analysis.  If you look long enough, you can always find something, even if it's not a real phenomenon.  You may want to consider applying the [Bonferroni correction](https://dataskeptic.com/blog/episodes/2016/bonferroni-correction) if appropriate for you.

If your problem has more features than observations, you almost certainly need to make some adjustment.  If you know your dataset very well, you as a domain expert might conduct a feature selection process in which you hand select a subset of variables you are confident contain most of the predictive power of the underlying dataset.  For example, when predicting, the weather, yesterday's temperature has a lot of value but the price of wheat is not likely to help your model much.

The most popular approach for an over-specified problem is to perform dimensionality reduction on your features, and then train a model against the lower dimensional space.  Principle components analysis (PCA) was historically the preferred choice.  PCA is a standard algorithm based on the minimization of variance.  It finds a way of mapping your data into another representation.  Once transformed, your data will no longer be directly interpretable, however, it will be more suitable set of inputs which can be truncated to lower dimensions with the minimized loss in the process.

With your data transformed to a smaller dimensional space, we can confidently train a machine learning model without worrying about having too many features.  Provided your dimensionality approach has captured the essence of the data, you'll likely build a reasonable model.

PCA is not the only approach for dimensionality reduction.  If anything, the trend is towards new techniques which we can broadly call "embeddings".  Whereas the PCA algorithm uses the minimization of statistical variance to come up with it's solution, calculating an embedding is a more intrinsically learned methodology.  Essentially, the problem is to compute a pair of functions: encoder and decoder.  The encoder should map your data from N dimensions down to k < N dimensions.  The decoder reverses this process.  In general, loss will occur.  However, if ML can find a pair of functions which are successful in translating data to a smaller size and back, then the encoded version (the embedding) is typically a rich representation to then train a model on.  The embedding approach has gained success in a variety of contexts including natural language processing and computer vision.

In contrast to data scientists with limited data, other organizations handle personally identifiable information directly and must be exceptionally careful in how it is handled.  In most large companies, security policies are designed with the assumption that any employee's account could be come compromised at any time.  Thus, in an effort to limit the potential blast radius of damage caused when a data scientist's password is stolen, access can be significantly limited.

Yet without access to real production data, the demands of a highly secure data infrastructure have been historically crippling to many machine learning groups.  How can one train a model without any data?

A common approach is the creation of mock data.  In fact, I've encountered many teams that go far down deep rabbit holes building mocking systems that create data based on some parameterized statistical model.  While those systems may have their uses, fake data is always fake data.  No set of randomization functions is going to produce data with the fidelity and variety of real world data.

Some organizations focus on using their true production data, but anonymizing it before exfiltrating it from the production system to a location where analytics can be done.  Anonymization of data is a reasonable option.  For effectively all good applications of data science, you never need precise PII.  You do need a notion of uniqueness.  While using a name, phone number, or email is a common tactic, a hashed version of the unique identifier yields an anonymous unique identifier.  Yet, there still remains some risk of de-identification that cannot be eliminated with this technique.

As time progresses, techniques and tools are beginning to emerge for fake data generation.  These systems can perform anonymizations of PII and offer a variety of other services.  For example, rather than (or in addition to) a strict normalization, it is possible to have generative systems that are able to create new fake observations that are inspired by production data.  This fake data should be distinct from true records, but is (ideally) indistinguishable from real data.  At an aggregate, the fake data should have statistically equivalent properties.  At the individual level, it should have plausible records that makes sense, like geo-graphic and area code being correlated, but not perfectly correlated.

Finally pre-trained models offer opportunities for assisting you when working on small datasets.  Take text analysis as an example.  With a few hundred free text responses, traditional techniques like term frequency counts are unlikely to give you state of the art results.  Yet, these free text strings could be encoded as embeddings using pre-trained models like BERT and GPT-3.  In tern that numeric representation of the text data can be surprisingly sufficient out-of-box for solving common natural language processing tasks of all kinds.

So when working with a small dataset, remember that all host is not lost.  A human domain expert is likely to be a necessary collaborator.  You need to be wary of overfitting, and may want to consider a feature selection technique, dimensionality reduction, or the use of pre-trained models.  In other cases, sophisticated approaches to fake data can be a great tool for generating enough robust examples to bootstrap your project or protect the privacy of your customers.

